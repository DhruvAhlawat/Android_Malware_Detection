{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction methods as functions with parameters are given below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Creating a Correlation matrix and detecting highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' returns a dataframe with all the correlated columns removed. (Only one of them left remaining)\n",
    "removals is used to remove important columns beforehand and then add them back later after the removal process. (to avoid removing important columns '''\n",
    "def Reduce_by_correlation(df, max_correlation= 0.9,  removals=[]):\n",
    "    rem = []\n",
    "    for name in removals:\n",
    "        rem.append(df[name]); \n",
    "        df = df.drop(name, axis=1); \n",
    "    cor_matrix = df.corr(numeric_only=True).abs()\n",
    "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > max_correlation)]\n",
    "    df = df.drop(df[to_drop], axis=1)\n",
    "    for i in range(0, len(rem)):\n",
    "        df.insert(i, removals[i], rem[i])    \n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Identifying features that have low support\n",
    "We know that features that have low support would not be much useful to us. \n",
    "Even then it may be important to keep features which only appear in malicious apps, even if with very low support as they may be good identifiers of some types of malware. For this we have a danger threshold which states that if the samples which have this feature have this much % of just malware apps, then we do not remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Reduce_by_support(df, min_support = 0.1, removals=[], label='isMalware'):\n",
    "    rem = []\n",
    "    supported_features = df; \n",
    "\n",
    "    dropping_features_through_support = []\n",
    "    totalMal = supported_features[label].sum()\n",
    "    totalBen = len(supported_features[label]) - totalMal; \n",
    "\n",
    "    for column in supported_features.columns:\n",
    "        if column != label and column not in removals:\n",
    "            curMal = supported_features[column][supported_features[label] == 1].sum()/totalMal\n",
    "            curBen = supported_features[column][supported_features[label] == 0].sum()/totalBen\n",
    "            total = curMal + curBen\n",
    "            if total < min_support and (curMal < 10*curBen or total < 0.9*min_support):\n",
    "                dropping_features_through_support.append(column)\n",
    "    \n",
    "    supported_features = supported_features.drop(dropping_features_through_support, axis=1)\n",
    "    \n",
    "    return supported_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Now, Applying a ranking method to the features so we can further reduce them . \n",
    "#### First we simply create the rankings, then decide the threshold uptill which we will accept the values\n",
    "the feature ranking is based on the values of score,\n",
    "- Score = (x-y)/(x+y). \n",
    "- x = % of malware samples that have the feature    \n",
    "- y = % of benign samples that have the feature.    \n",
    "\n",
    "the higher the absolute value of the score (from 0 to 1), the higher it is in importance for our machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(all='raise')\n",
    "    \n",
    "def Reduce_by_score(df, score_threshold = 0.05,percent_retain = 0.9, removals = [], label = 'isMalware'):\n",
    "    score_dropouts = []\n",
    "    rem = []\n",
    "    rankreduced_features = df\n",
    "    # score_threshold = 0.2; #if the score is below this, then this MAY turn out to be not important. \n",
    "    allScores = []\n",
    "\n",
    "    totalMal = rankreduced_features['isMalware'].sum()\n",
    "    totalBen = len(rankreduced_features['isMalware']) - totalMal; \n",
    "    for column in rankreduced_features.columns:\n",
    "        if(column == label or column in removals or column == \"Name\" or column == \"Activities\" or column == \"Services\" or \n",
    "        column == \"Permissions\" or column == \"Actions\" or column == \"receivers\" or column == \"Categories\" ):\n",
    "            continue\n",
    "  \n",
    "        malSum = rankreduced_features[column][rankreduced_features[label] == 1].sum()\n",
    "        benSum = rankreduced_features[column][rankreduced_features[label] == 0].sum()\n",
    "        \n",
    "        malPercentage = malSum/totalMal\n",
    "        benPercentage = benSum/totalBen\n",
    "        try:\n",
    "            score = (malPercentage - benPercentage)/(malPercentage + benPercentage)\n",
    "            allScores.append((score, column))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(column)\n",
    "            print(\"malSum: \", malSum, \"benSum: \", benSum)\n",
    "            # raise e; \n",
    "            \n",
    "        \n",
    "\n",
    "    allScores = sorted(allScores, key=lambda x: abs(x[0]), reverse=True)\n",
    "    i = 0; \n",
    "    size = len(allScores); \n",
    "    for score, col in allScores:\n",
    "        if(abs(score) < score_threshold or i > percent_retain * size):\n",
    "            score_dropouts.append(col) #removing this feature in this case.\n",
    "        i += 1; \n",
    "\n",
    "    rankreduced_features = rankreduced_features.drop(score_dropouts, axis=1)\n",
    "    return rankreduced_features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating multiple experimental featuresets, and later testing accuracy on all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734, 11388)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDF = pd.read_csv('~/Documents/Android_Malware_Analysis/Data/ListFeatures/ManifestFeatures.csv')\n",
    "Label = 'isMalware' #the label column name, may also be Labels\n",
    "Reduced = {}\n",
    "Reduced['Original'] = originalDF\n",
    "originalDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 (734, 106)\n",
      "0.075 (734, 130)\n",
      "0.05 (734, 163)\n"
     ]
    }
   ],
   "source": [
    "Reduced['Support'] = {}\n",
    "# We should not go above 0.1. 10% is already quite a lot.\n",
    "Reduced['Support']['0.1'] = Reduce_by_support(originalDF, min_support=0.1, removals=['Name'])\n",
    "Reduced['Support']['0.075'] = Reduce_by_support(originalDF, min_support=0.075, removals=['Name'])\n",
    "Reduced['Support']['0.05'] = Reduce_by_support(originalDF, min_support=0.05, removals=['Name'])\n",
    "\n",
    "\n",
    "for name in Reduced['Support']:\n",
    "    print(name, Reduced['Support'][name].shape)\n",
    "    #we also save the corresponding CSV files \n",
    "    Reduced['Support'][name].to_csv('./Data/Experimentation/Support/Support'+name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 (734, 8544)\n",
      "0.15 (734, 9113)\n",
      "0.1 (734, 9682)\n",
      "0.05 (734, 10251)\n"
     ]
    }
   ],
   "source": [
    "Reduced['Score'] = {}\n",
    "# 0.8 is probably toO high, but lets see. \n",
    "Reduced['Score']['0.25'] = Reduce_by_score(originalDF, score_threshold=0.25, percent_retain=0.75, removals=['Name'])\n",
    "Reduced['Score']['0.15'] = Reduce_by_score(originalDF, score_threshold=0.15, percent_retain=0.8, removals=['Name'])\n",
    "Reduced['Score']['0.1'] = Reduce_by_score(originalDF, score_threshold=0.1, percent_retain=0.85, removals=['Name'])\n",
    "Reduced['Score']['0.05'] = Reduce_by_score(originalDF, score_threshold=0.05, removals=['Name'])\n",
    "\n",
    "for name in Reduced['Score']:\n",
    "    print(name, Reduced['Score'][name].shape)\n",
    "    #we also save the corresponding CSV files \n",
    "    Reduced['Score'][name].to_csv('./Data/Experimentation/Score/Score'+name+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Applying the reduction methods by stacking them on top of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1_0.75 (734, 92)\n",
      "0.1_0.8 (734, 101)\n",
      "0.1_0.85 (734, 110)\n",
      "0.1_0.9 (734, 112)\n",
      "0.1_0.95 (734, 117)\n",
      "0.075_0.75 (734, 111)\n",
      "0.075_0.8 (734, 121)\n",
      "0.075_0.85 (734, 130)\n",
      "0.075_0.9 (734, 133)\n",
      "0.075_0.95 (734, 139)\n",
      "0.05_0.75 (734, 150)\n",
      "0.05_0.8 (734, 159)\n",
      "0.05_0.85 (734, 169)\n",
      "0.05_0.9 (734, 175)\n",
      "0.05_0.95 (734, 185)\n"
     ]
    }
   ],
   "source": [
    "Reduced['SupportCorrelation'] = {}\n",
    "CorrelationList = ['0.75','0.8', '0.85', '0.9', '0.95']\n",
    "for arg0 in Reduced['Support']:\n",
    "    for arg1 in CorrelationList:\n",
    "        Reduced['SupportCorrelation'][arg0+'_'+arg1] = Reduce_by_correlation(Reduced['Support'][arg0], max_correlation=float(arg1))\n",
    "\n",
    "for name in Reduced['SupportCorrelation']:\n",
    "    print(name, Reduced['SupportCorrelation'][name].shape)\n",
    "    #we also save the corresponding CSV files \n",
    "    Reduced['SupportCorrelation'][name].to_csv('./Data/Experimentation/CorrelationSupport/SupportCorrelation'+name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1_0.75_0.2 (734, 77)\n",
      "0.1_0.75_0.4 (734, 67)\n",
      "0.1_0.75_0.6 (734, 57)\n",
      "0.1_0.8_0.2 (734, 86)\n",
      "0.1_0.8_0.4 (734, 76)\n",
      "0.1_0.8_0.6 (734, 64)\n",
      "0.1_0.85_0.2 (734, 94)\n",
      "0.1_0.85_0.4 (734, 82)\n",
      "0.1_0.85_0.6 (734, 70)\n",
      "0.1_0.9_0.2 (734, 96)\n",
      "0.1_0.9_0.4 (734, 83)\n",
      "0.1_0.9_0.6 (734, 71)\n",
      "0.1_0.95_0.2 (734, 101)\n",
      "0.1_0.95_0.4 (734, 88)\n",
      "0.1_0.95_0.6 (734, 75)\n",
      "0.075_0.75_0.2 (734, 93)\n",
      "0.075_0.75_0.4 (734, 80)\n",
      "0.075_0.75_0.6 (734, 67)\n",
      "0.075_0.8_0.2 (734, 103)\n",
      "0.075_0.8_0.4 (734, 90)\n",
      "0.075_0.8_0.6 (734, 75)\n",
      "0.075_0.85_0.2 (734, 111)\n",
      "0.075_0.85_0.4 (734, 96)\n",
      "0.075_0.85_0.6 (734, 81)\n",
      "0.075_0.9_0.2 (734, 114)\n",
      "0.075_0.9_0.4 (734, 98)\n",
      "0.075_0.9_0.6 (734, 83)\n",
      "0.075_0.95_0.2 (734, 120)\n",
      "0.075_0.95_0.4 (734, 104)\n",
      "0.075_0.95_0.6 (734, 88)\n",
      "0.05_0.75_0.2 (734, 130)\n",
      "0.05_0.75_0.4 (734, 113)\n",
      "0.05_0.75_0.6 (734, 94)\n",
      "0.05_0.8_0.2 (734, 139)\n",
      "0.05_0.8_0.4 (734, 122)\n",
      "0.05_0.8_0.6 (734, 101)\n",
      "0.05_0.85_0.2 (734, 148)\n",
      "0.05_0.85_0.4 (734, 129)\n",
      "0.05_0.85_0.6 (734, 108)\n",
      "0.05_0.9_0.2 (734, 154)\n",
      "0.05_0.9_0.4 (734, 134)\n",
      "0.05_0.9_0.6 (734, 112)\n",
      "0.05_0.95_0.2 (734, 164)\n",
      "0.05_0.95_0.4 (734, 144)\n",
      "0.05_0.95_0.6 (734, 121)\n"
     ]
    }
   ],
   "source": [
    "Reduced['SupportCorrelationScore'] = {}\n",
    "for name in Reduced['SupportCorrelation']:\n",
    "    for i in range(2,7,2):\n",
    "        k = i/10; \n",
    "        Reduced['SupportCorrelationScore'][name+'_'+str(k)] = Reduce_by_score(Reduced['SupportCorrelation'][name], score_threshold=k, removals=['Name'])\n",
    "    #we also save the corresponding CSV files \n",
    "    \n",
    "if(os.path.isdir('./Data/Experimentation/SupportCorrelationScore') == False):\n",
    "    os.mkdir('./Data/Experimentation/SupportCorrelationScore')\n",
    "for name in Reduced['SupportCorrelationScore']:\n",
    "    print(name, Reduced['SupportCorrelationScore'][name].shape)\n",
    "    #we also save the corresponding CSV files \n",
    "    Reduced['SupportCorrelationScore'][name].to_csv('./Data/Experimentation/SupportCorrelationScore/SupportCorrelationScore'+name+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Visualising one of the featuresets that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedFeatureSet = 0;\n",
    "try:\n",
    "    reducedFeatureSet = Reduced['CorrelationSupportScore']['0.75_0.075_0.6']\n",
    "except Exception as e:\n",
    "    reducedFeatureSet = pd.read_csv('./Data/Experimentation/CorrelationSupportScore/CorrelationSupportScore0.75_0.075_0.6.csv')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "totalMal = reducedFeatureSet['isMalware'].sum(); \n",
    "totalBen = len(reducedFeatureSet['isMalware']) - totalMal; \n",
    "featureCount = []\n",
    "for column in reducedFeatureSet.columns:\n",
    "    if(column == \"isMalware\" or column == \"Name\" ):\n",
    "        continue\n",
    "    \n",
    "    malSum = 0; \n",
    "    benSum = 0;   \n",
    "    curCol = reducedFeatureSet[column]\n",
    "    for i in range(0, len(curCol)):\n",
    "        if(curCol[i] == 1 and reducedFeatureSet['isMalware'][i] == 1):\n",
    "            malSum += 1\n",
    "        elif(curCol[i] == 1 and reducedFeatureSet['isMalware'][i] == 0):\n",
    "            benSum += 1\n",
    "    \n",
    "    malPercentage = malSum/totalMal\n",
    "    benPercentage = benSum/totalBen\n",
    "    featureCount.append((column, malPercentage, benPercentage))\n",
    "\n",
    "featureCount = sorted(featureCount, key=lambda x: (x[1] - x[2]), reverse=True)\n",
    "df = pd.DataFrame(featureCount, columns=['Feature', 'Malware', 'Benign'])\n",
    "benFC = []\n",
    "for i,j,k in featureCount:\n",
    "    benFC.append((i,j,\"Malware\"))\n",
    "    benFC.append((i,k,\"Benign\"))\n",
    "\n",
    "    \n",
    "df2 = pd.DataFrame(benFC, columns=['Feature', 'Count', 'isMalware'])\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.barplot(ax=ax,data=df2 ,y='Feature',x = 'Count' , hue='isMalware')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
